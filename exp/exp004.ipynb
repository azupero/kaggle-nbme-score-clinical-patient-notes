{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"3rOHeiHoOKPe","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1648794088425,"user_tz":-540,"elapsed":18,"user":{"displayName":"永友遥","userId":"11743586908271963047"}},"outputId":"fb664248-1690-4a65-dee3-601ef49f6c68"},"outputs":[{"output_type":"stream","name":"stdout","text":["Fri Apr  1 06:21:28 2022       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   32C    P0    22W / 300W |      0MiB / 16160MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}],"source":["!nvidia-smi"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"78XXjkCdOOzD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1648794109633,"user_tz":-540,"elapsed":21213,"user":{"displayName":"永友遥","userId":"11743586908271963047"}},"outputId":"fcc94ae0-5602-4f37-bec6-c58a96aaf498"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[K     |████████████████████████████████| 582 kB 13.7 MB/s \n","\u001b[K     |████████████████████████████████| 1.7 MB 75.7 MB/s \n","\u001b[K     |████████████████████████████████| 398 kB 87.9 MB/s \n","\u001b[K     |████████████████████████████████| 3.8 MB 82.3 MB/s \n","\u001b[K     |████████████████████████████████| 1.2 MB 71.4 MB/s \n","\u001b[K     |████████████████████████████████| 596 kB 84.4 MB/s \n","\u001b[K     |████████████████████████████████| 136 kB 60.7 MB/s \n","\u001b[K     |████████████████████████████████| 1.1 MB 66.3 MB/s \n","\u001b[K     |████████████████████████████████| 144 kB 86.4 MB/s \n","\u001b[K     |████████████████████████████████| 181 kB 78.0 MB/s \n","\u001b[K     |████████████████████████████████| 63 kB 2.0 MB/s \n","\u001b[K     |████████████████████████████████| 6.6 MB 58.9 MB/s \n","\u001b[K     |████████████████████████████████| 67 kB 5.3 MB/s \n","\u001b[K     |████████████████████████████████| 895 kB 62.6 MB/s \n","\u001b[K     |████████████████████████████████| 271 kB 75.0 MB/s \n","\u001b[K     |████████████████████████████████| 144 kB 92.0 MB/s \n","\u001b[K     |████████████████████████████████| 94 kB 3.3 MB/s \n","\u001b[?25h  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","tensorflow 2.8.0 requires tf-estimator-nightly==2.8.0.dev2021122109, which is not installed.\n","arviz 0.11.4 requires typing-extensions<4,>=3.7.4.3, but you have typing-extensions 4.1.1 which is incompatible.\u001b[0m\n","\u001b[K     |████████████████████████████████| 58 kB 4.6 MB/s \n","\u001b[?25h  Building wheel for kaggle (setup.py) ... \u001b[?25l\u001b[?25hdone\n"]}],"source":["!pip install -q pytorch-lightning wandb torchmetrics transformers sentencepiece\n","!pip install -q --upgrade --force-reinstall --no-deps kaggle"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"nQmGCwMMOREH","executionInfo":{"status":"ok","timestamp":1648794110755,"user_tz":-540,"elapsed":1127,"user":{"displayName":"永友遥","userId":"11743586908271963047"}}},"outputs":[],"source":["!mkdir /root/.kaggle\n","!cp /content/drive/MyDrive/Colab/kaggle/kaggle.json /root/.kaggle/kaggle.json"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":9392,"status":"ok","timestamp":1648794297868,"user":{"displayName":"永友遥","userId":"11743586908271963047"},"user_tz":-540},"id":"tFUGUD38OVhD"},"outputs":[],"source":["import os\n","import gc\n","import sys\n","import json\n","import itertools\n","from tqdm.auto import tqdm\n","import logging\n","import datetime\n","import ast\n","\n","import numpy as np\n","import pandas as pd\n","import sklearn.model_selection as sms\n","from sklearn.metrics import f1_score\n","import math\n","import re\n","\n","import torch\n","import torch.nn as nn\n","from torch.nn import functional as F\n","from torch.utils.data import Dataset, DataLoader\n","import torch.optim as optim\n","\n","import pytorch_lightning as pl\n","from pytorch_lightning import Trainer, seed_everything\n","from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping, LearningRateMonitor\n","from pytorch_lightning.loggers import WandbLogger\n","\n","from transformers import AutoConfig, AutoModel, AutoTokenizer, get_cosine_schedule_with_warmup, get_linear_schedule_with_warmup\n","\n","import wandb"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":574,"status":"ok","timestamp":1648794979416,"user":{"displayName":"永友遥","userId":"11743586908271963047"},"user_tz":-540},"id":"1duE-kW_OZZq"},"outputs":[],"source":["class Config:\n","    # ==============================\n","    # Globals #\n","    # ==============================\n","    competition_name = \"nbme-score-clinical-patient-notes\"\n","    group = \"RoBERTa-large\"\n","    exp_id = \"004\"\n","    debug = False\n","    inference_only = True\n","    upload_from_colab = False\n","    colab_dir = \"/content/drive/MyDrive/Colab/kaggle/nbme-score-clinical-patient-notes\"\n","    kaggle_json_path = \"/root/.kaggle/kaggle.json\"\n","    kaggle_dataset_path = None\n","    gpus = 1\n","    seed = 2434\n","    max_epochs = 5\n","    accumulate_grad_batches = 2\n","    precision = 32\n","    num_fold = 5\n","    train_fold = [0,1,2,3,4] # 実行するfold\n","    # ==============================\n","    # Dataloader #\n","    # ==============================\n","    train_batch_size = 4\n","    valid_batch_size = 32\n","    test_batch_size = 32\n","    num_workers = 8\n","    # ==============================\n","    # Split #\n","    # ==============================\n","    split_name = \"GroupKFold\"\n","    split_params = {\n","        \"n_splits\": num_fold if not debug else 4,\n","        # \"shuffle\": True,\n","        # \"random_state\": seed,\n","    }\n","    # ==============================\n","    # Model #\n","    # ==============================\n","    model_name = \"roberta-large\"\n","    max_length = 512\n","    hidden_size = 1024\n","    num_class = 1\n","    dropout = 0.2\n","    initializer_range = 0.02\n","    # ==============================\n","    # Loss #\n","    # ==============================\n","    loss_name = \"BCEWithLogitsLoss\"\n","    loss_params = {\n","        \"reduction\": \"none\"\n","    }\n","    # ==============================\n","    # Optimizer #\n","    # ==============================\n","    optimizer_name = \"AdamW\"\n","    optimizer_params = {\n","        \"lr\": 1e-4,\n","        \"weight_decay\": 1e-2,\n","        \"eps\": 1e-6,\n","        \"betas\": (0.9, 0.999)\n","    }\n","    encoder_lr = 2e-5\n","    decoder_lr = 2e-5\n","    weight_decay = 0.01\n","    # ==============================\n","    # Scheduler #\n","    # ==============================\n","    scheduler_name = \"cosine-warmup\"\n","    scheduler_warmup_ratio = 0.1\n","    scheduler_params = {}\n","    scheduler_interval = \"step\"\n","    # ==============================\n","    # Callbacks #\n","    # ==============================\n","    checkpoint_params = {\n","        \"monitor\": \"val/micro-F1\",\n","        \"save_top_k\": 1,\n","        \"save_weights_only\": True,\n","        \"mode\": \"max\",\n","        \"verbose\": True,\n","    }\n","    early_stopping = False\n","    early_stopping_params = {\n","        \"monitor\": \"val/loss\",\n","        \"min_delta\": 0.0,\n","        \"patience\": 8,\n","        \"verbose\": False,\n","        \"mode\": \"min\",\n","    }"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":4421,"status":"ok","timestamp":1648794302283,"user":{"displayName":"永友遥","userId":"11743586908271963047"},"user_tz":-540},"id":"sokiSfvKSjft"},"outputs":[],"source":["# ====================================\n","# Setup #\n","# ====================================\n","class Logger:\n","    \"\"\" ref) https://github.com/ghmagazine/kagglebook/blob/master/ch04-model-interface/code/util.py\"\"\"\n","    def __init__(self, path):\n","        self.general_logger = logging.getLogger(path)\n","        stream_handler = logging.StreamHandler()\n","        file_general_handler = logging.FileHandler(os.path.join(path, 'Experiment.log'))\n","        if len(self.general_logger.handlers) == 0:\n","            self.general_logger.addHandler(stream_handler)\n","            self.general_logger.addHandler(file_general_handler)\n","            self.general_logger.setLevel(logging.INFO)\n","\n","    def info(self, message):\n","        # display time\n","        self.general_logger.info('[{}] - {}'.format(self.now_string(), message))\n","\n","    @staticmethod\n","    def now_string():\n","        return str(datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n","\n","\n","def setup(cfg):\n","    cfg.on_colab = \"google.colab\" in sys.modules\n","    if cfg.on_colab:\n","        # kaggle api\n","        f = open(Config.kaggle_json_path, 'r')\n","        json_data = json.load(f)\n","        os.environ[\"KAGGLE_USERNAME\"] = json_data[\"username\"]\n","        # set input/output dir\n","        cfg.input_dir = os.path.join(cfg.colab_dir, \"input\")\n","        cfg.train_csv = os.path.join(cfg.input_dir, \"cleaned_train.csv\")\n","        cfg.features_csv = os.path.join(cfg.input_dir, \"features.csv\")\n","        cfg.patient_notes_csv = os.path.join(cfg.input_dir, \"patient_notes.csv\")\n","        cfg.test_csv = os.path.join(cfg.input_dir, \"test.csv\")\n","        cfg.sample_submission = os.path.join(cfg.input_dir, \"sample_submission.csv\")\n","        cfg.output_dir = os.path.join(cfg.colab_dir, \"output\")\n","        cfg.exp_output_dir = os.path.join(cfg.output_dir, f\"exp{cfg.exp_id}\")\n","        cfg.model_dir = os.path.join(cfg.exp_output_dir, \"model\")\n","\n","        for d in [cfg.output_dir, cfg.exp_output_dir, cfg.model_dir]:\n","            os.makedirs(d, exist_ok=True)\n","            \n","        # wandb\n","        wandb.login()\n","    else:\n","        cfg.input_dir = f\"../input/{cfg.competition_name}\"\n","        cfg.train_csv = os.path.join(cfg.input_dir, \"train.csv\")\n","        cfg.features_csv = os.path.join(cfg.input_dir, \"features.csv\")\n","        cfg.patient_notes_csv = os.path.join(cfg.input_dir, \"patient_notes.csv\")\n","        cfg.test_csv = os.path.join(cfg.input_dir, \"test.csv\")\n","        cfg.sample_submission = os.path.join(cfg.input_dir, \"sample_submission.csv\")\n","        cfg.submission = \"./\"\n","        cfg.exp_output_dir = f\"exp{cfg.exp_id}\"\n","        cfg.model_dir = os.path.join(cfg.exp_output_dir, \"model\")\n","\n","        if cfg.kaggle_dataset_path is not None:\n","            cfg.model_dir = os.path.join(cfg.kaggle_dataset_path, \"model\")\n","\n","        for d in [cfg.exp_output_dir, cfg.model_dir]:\n","            os.makedirs(d, exist_ok=True)\n","\n","    return cfg\n","\n","\n","# ====================================\n","# Preprocess #\n","# ====================================\n","def get_input_data(cfg, input_type=\"train\"):\n","    input_df = pd.read_csv(cfg.train_csv) if input_type == \"train\" else pd.read_csv(cfg.test_csv)\n","    if cfg.debug and input_type != \"test\":\n","        input_df = input_df[input_df[\"pn_num\"].isin(input_df[\"pn_num\"].unique()[:100])].reset_index(drop=True)\n","    \n","    feature_texts_df = pd.read_csv(Config.features_csv)\n","    patient_notes_df = pd.read_csv(Config.patient_notes_csv)\n","\n","    if input_type == \"train\":\n","        input_df[\"annotation\"] = input_df[\"annotation\"].apply(ast.literal_eval)\n","        input_df[\"location\"] = input_df[\"location\"].apply(ast.literal_eval)\n","    \n","    input_df = input_df.merge(feature_texts_df, on=[\"feature_num\", \"case_num\"], how=\"left\")\n","    input_df = input_df.merge(patient_notes_df, on=[\"pn_num\", \"case_num\"], how=\"left\")\n","\n","    input_df[\"pn_history\"] = input_df[\"pn_history\"].apply(clean_feature_text_for_preprocess)\n","\n","    return input_df\n","\n","\n","def get_split(cfg, train_df):\n","    split_name = cfg.split_name\n","    split_params = cfg.split_params\n","    splitter = sms.__getattribute__(split_name)(**split_params)\n","\n","    groups = train_df[\"pn_num\"].to_numpy()\n","    train_df[\"fold\"] = -1\n","\n","    for fold_id, (train_idx, valid_idx) in enumerate(splitter.split(train_df, train_df[\"location\"], groups)):\n","        train_df.loc[valid_idx, \"fold\"] = int(fold_id)\n","\n","    return train_df\n","\n","\n","def get_filname_listdir(dirctory):\n","    listdir = os.listdir(dirctory)\n","    out_lst = [os.path.splitext(d)[0] for d in listdir]\n","    return out_lst\n","\n","\n","def get_tokenizer(cfg):\n","    if cfg.kaggle_dataset_path is not None:\n","        pretrained_dir = os.path.join(cfg.kaggle_dataset_path, \"pretrain_tokenizer\")\n","    else:\n","        pretrained_dir = os.path.join(cfg.exp_output_dir, \"pretrain_tokenizer\")\n","\n","    if not os.path.isdir(pretrained_dir):\n","        if \"roberta\" not in cfg.model_name:\n","            tokenizer = AutoTokenizer.from_pretrained(cfg.model_name)\n","        else:\n","            tokenizer = AutoTokenizer.from_pretrained(cfg.model_name, trim_offsets=False)\n","\n","        tokenizer.save_pretrained(pretrained_dir)\n","\n","    else:\n","        if \"roberta\" not in cfg.model_name:\n","            tokenizer = AutoTokenizer.from_pretrained(pretrained_dir)\n","        else:\n","            tokenizer = AutoTokenizer.from_pretrained(pretrained_dir, trim_offsets=False)\n","\n","    return tokenizer\n","\n","\n","def get_backbone(cfg):\n","    if cfg.kaggle_dataset_path is not None:\n","        pretrained_dir = os.path.join(cfg.kaggle_dataset_path, \"pretrain_model\")\n","    else:\n","        pretrained_dir = os.path.join(cfg.exp_output_dir, \"pretrain_model\")\n","\n","    if not os.path.isdir(pretrained_dir):\n","        model_config = AutoConfig.from_pretrained(cfg.model_name)\n","        backbone = AutoModel.from_pretrained(cfg.model_name, config=model_config)\n","\n","        backbone.save_pretrained(pretrained_dir)\n","\n","    else:\n","        model_config = AutoConfig.from_pretrained(pretrained_dir)\n","        backbone = AutoModel.from_pretrained(pretrained_dir, config=model_config)\n","\n","    return backbone\n","\n","\n","def clean_feature_text_for_preprocess(text: str):\n","    \"\"\"\n","    reference: https://www.kaggle.com/code/theoviel/roberta-strikes-back\n","    \"\"\"\n","    text = re.sub('I-year', '1-year', text)\n","    text = re.sub('-OR-', \" or \", text)\n","    text = re.sub('-', ' ', text)\n","\n","    return text\n","\n","\n","# ====================================\n","# Dataset #\n","# ====================================\n","def get_inputs(cfg, text: str, feature_text: str, tokenizer):\n","    encoding = tokenizer(\n","        text,\n","        feature_text,\n","        max_length=cfg.max_length,\n","        padding=\"max_length\",\n","        return_offsets_mapping=False,\n","        # add_special_tokens=True\n","    )\n","\n","    for k, v in encoding.items():\n","        encoding[k] = torch.tensor(v, dtype=torch.long)\n","\n","    return encoding\n","\n","\n","def get_label(cfg, text: str, locations: list, tokenizer):\n","    encoding = tokenizer(\n","        text,\n","        max_length=cfg.max_length,\n","        padding=\"max_length\",\n","        return_offsets_mapping=True,\n","        # add_special_tokens=True\n","    )\n","    \n","    offset_mapping = encoding[\"offset_mapping\"]\n","    ignore_idx = np.where(np.array(encoding.sequence_ids()) != 0)[0]\n","    label = np.zeros(len(offset_mapping))\n","    label[ignore_idx] = -1\n","\n","    if len(locations) != 0:\n","        for location in locations:\n","            for loc in [s.split() for s in location.split(\";\")]:\n","                start_idx = -1\n","                end_idx = -1\n","                start, end = int(loc[0]), int(loc[1])\n","                for idx in range(len(offset_mapping)):\n","                    # DeBERTaのTokenizerは前の空白も含めるため+1する\n","                    if (start_idx == -1) & (start < offset_mapping[idx][0]):\n","                        start_idx = idx - 1\n","                    if (end_idx == -1) & (end <= offset_mapping[idx][1]):\n","                        end_idx = idx + 1\n","                if start_idx == -1:\n","                    start_idx = end_idx\n","                if (start_idx != -1) & (end_idx != -1):\n","                    label[start_idx: end_idx] = 1\n","    \n","    return torch.tensor(label, dtype=torch.float)\n","\n","\n","class NBMEDataset(Dataset):\n","    def __init__(self, cfg, input_df: pd.DataFrame, tokenizer, phase: str = \"train\"):\n","        self.cfg = cfg\n","        self.input_df = input_df\n","        self.tokenizer = tokenizer\n","        self.phase = phase\n","        self.pn_histories = self.input_df[\"pn_history\"].to_numpy()\n","        self.feature_texts = self.input_df[\"feature_text\"].to_numpy()\n","        self.locations = self.input_df[\"location\"].to_numpy() if self.phase is \"train\" else None\n","\n","    def __len__(self):\n","        return len(self.input_df)\n","\n","    def __getitem__(self, idx):\n","        if self.phase == \"train\":\n","            inputs = get_inputs(\n","                self.cfg,\n","                self.pn_histories[idx],\n","                self.feature_texts[idx],\n","                self.tokenizer,\n","            )\n","            label = get_label(\n","                self.cfg,\n","                self.pn_histories[idx],\n","                self.locations[idx],\n","                self.tokenizer,\n","            )\n","\n","            return {\n","                \"input_ids\": inputs[\"input_ids\"],\n","                \"attention_mask\": inputs[\"attention_mask\"],\n","                \"labels\": label,\n","            }\n","\n","        elif self.phase == \"test\":\n","            inputs = get_inputs(\n","                self.cfg,\n","                self.pn_histories[idx],\n","                self.feature_texts[idx],\n","                self.tokenizer,\n","            )\n","\n","            return {\n","                \"input_ids\": inputs[\"input_ids\"],\n","                \"attention_mask\": inputs[\"attention_mask\"],\n","            }\n","        else:\n","            raise NotImplementedError\n","\n","\n","class NBMEDataModule(pl.LightningDataModule):\n","    def __init__(self, cfg, tokenizer, train_df: pd.DataFrame = None, valid_df: pd.DataFrame = None, test_df: pd.DataFrame = None):\n","        super(NBMEDataModule, self).__init__()\n","\n","        self.cfg = cfg\n","        self.tokenizer = tokenizer\n","        self.train_df = train_df\n","        self.valid_df = valid_df\n","        self.test_df = test_df\n","\n","    def prepare_data(self):\n","        if self.test_df is None:\n","            self.train_dataset = NBMEDataset(\n","                cfg=self.cfg,\n","                input_df=self.train_df,\n","                tokenizer=self.tokenizer,\n","                phase=\"train\"\n","            )\n","            self.val_dataset = NBMEDataset(\n","                cfg=self.cfg,\n","                input_df=self.valid_df,\n","                tokenizer=self.tokenizer,\n","                phase=\"train\"\n","            )\n","        else:\n","            self.test_dataset = NBMEDataset(\n","                cfg=self.cfg,\n","                input_df=self.test_df,\n","                tokenizer=self.tokenizer,\n","                phase=\"test\"\n","            )\n","\n","    def train_dataloader(self):\n","        return DataLoader(\n","            self.train_dataset,\n","            batch_size=self.cfg.train_batch_size,\n","            num_workers=self.cfg.num_workers,\n","            shuffle=True,\n","            pin_memory=True,\n","            drop_last=False,\n","        )\n","    \n","    def val_dataloader(self):\n","        return DataLoader(\n","            self.val_dataset,\n","            batch_size=self.cfg.valid_batch_size,\n","            num_workers=self.cfg.num_workers,\n","            shuffle=False,\n","            pin_memory=True,\n","            drop_last=False,\n","        )\n","\n","    def predict_dataloader(self):\n","        return DataLoader(\n","            self.test_dataset,\n","            batch_size=self.cfg.test_batch_size,\n","            num_workers=self.cfg.num_workers,\n","            shuffle=False,\n","            pin_memory=True,\n","            drop_last=False,\n","        )\n","\n","\n","# ====================================\n","# Model #\n","# ====================================\n","class NBMEModel(nn.Module):\n","    def __init__(self, cfg):\n","        super(NBMEModel, self).__init__()\n","\n","        self.cfg = cfg\n","        self.backbone = get_backbone(self.cfg)\n","        self.dropout = nn.Dropout(self.cfg.dropout)\n","        self.classifier = nn.Linear(self.cfg.hidden_size, self.cfg.num_class)\n","        self._init_weights(self.classifier)\n","\n","    def _init_weights(self, module):\n","        if isinstance(module, nn.Linear):\n","            module.weight.data.normal_(mean=0.0, std=self.cfg.initializer_range)\n","            if module.bias is not None:\n","                module.bias.data.zero_()\n","        elif isinstance(module, nn.Embedding):\n","            module.weight.data.normal_(mean=0.0, std=self.cfg.initializer_range)\n","            if module.padding_idx is not None:\n","                module.weight.data[module.padding_idx].zero_()\n","        elif isinstance(module, nn.LayerNorm):\n","            module.bias.data.zero_()\n","            module.weight.data.fill_(1.0)\n","\n","    def forward(self, input_ids, attention_mask=None):\n","        outputs = self.backbone(input_ids=input_ids, attention_mask=attention_mask) # (batch_size, seq_len, hidden_size)\n","        x = outputs[0] # extract last_hidden_states\n","        x = self.dropout(x)\n","        x = self.classifier(x) # (batch_size, seq_len, num_class)\n","\n","        return x\n","\n","\n","class NBMELightningModule(pl.LightningModule):\n","    def __init__(self, cfg, tokenizer=None, valid_df=None, valid_labels=None):\n","        super(NBMELightningModule, self).__init__()\n","\n","        self.cfg = cfg\n","        self.model = NBMEModel(self.cfg)\n","        self.criterion = get_criterion(self.cfg)\n","        self.tokenizer = tokenizer\n","        self.valid_df = valid_df\n","        self.valid_labels = valid_labels\n","\n","    def setup(self, stage=None):\n","        # calculate training total steps\n","        if stage == \"fit\":\n","            self.training_steps = math.ceil(len(self.trainer.datamodule.train_dataloader()) / self.trainer.accumulate_grad_batches) * self.trainer.max_epochs\n","            self.warmup_steps = int(self.training_steps * self.cfg.scheduler_warmup_ratio) if self.cfg.scheduler_warmup_ratio else None\n","    \n","    def forward(self, input_ids, attention_mask):\n","        return self.model(input_ids, attention_mask)\n","\n","    def training_step(self, batch, batch_idx):\n","        input_ids, attention_mask, labels = batch[\"input_ids\"], batch[\"attention_mask\"], batch[\"labels\"]\n","        y_preds = self.forward(input_ids, attention_mask)\n","        loss = self.criterion(y_preds.view(-1, 1), labels.view(-1, 1))\n","        mask = (labels.view(-1, 1) != -1)\n","        loss = torch.masked_select(loss, mask).mean()\n","        self.log(\"train/loss\", loss, on_step=True, on_epoch=True, logger=True, prog_bar=True)\n","\n","        return loss\n","\n","    def validation_step(self, batch, batch_idx):\n","        input_ids, attention_mask, labels = batch[\"input_ids\"], batch[\"attention_mask\"], batch[\"labels\"]\n","        y_preds = self.forward(input_ids, attention_mask)\n","        loss = self.criterion(y_preds.view(-1, 1), labels.view(-1, 1))\n","        mask = (labels.view(-1, 1) != -1)\n","        loss = torch.masked_select(loss, mask).mean()\n","        self.log(\"val/loss\", loss, on_step=True, on_epoch=True, logger=True, prog_bar=True)\n","\n","        return {\n","            \"loss\": loss,\n","            \"preds\": y_preds.detach()\n","        }\n","\n","    def validation_epoch_end(self, outputs):\n","        preds = torch.cat([output[\"preds\"] for output in outputs]).squeeze().cpu().numpy()\n","        char_preds = get_token_probs_to_char_probs(self.valid_df[\"pn_history\"].to_numpy(), preds, self.tokenizer)\n","        results = get_results(char_preds, th=0.5)\n","        preds = get_predictions(results)\n","        score = get_score(self.valid_labels, preds)\n","        self.log(\"val/micro-F1\", score, logger=True, prog_bar=True)\n","\n","    def predict_step(self, batch, batch_idx, dataloader_idx=None):\n","        input_ids, attention_mask = batch[\"input_ids\"], batch[\"attention_mask\"]\n","        y_preds = self.forward(input_ids, attention_mask)\n","        y_preds = y_preds.sigmoid()\n","\n","        return y_preds.squeeze()\n","\n","    def configure_optimizers(self):\n","        optimizer_params = get_optimizer_params(self.model, self.cfg.encoder_lr, self.cfg.decoder_lr, self.cfg.weight_decay)\n","        optimizer = get_optimizer(self.cfg, optimizer_params)\n","\n","        if self.cfg.scheduler_name is None:\n","            return [optimizer]\n","        else:\n","            scheduler = get_scheduler(self.cfg, optimizer, num_warmup_steps=self.warmup_steps, num_training_steps=self.training_steps)\n","            scheduler = {\"scheduler\": scheduler, \"interval\": self.cfg.scheduler_interval}\n","\n","            return [optimizer], [scheduler]\n","\n","\n","# ====================================\n","# Criterion, Optimizer, Scheduler #\n","# ====================================\n","def get_criterion(cfg):\n","    loss_name = cfg.loss_name\n","    loss_params = cfg.loss_params\n","\n","    return nn.__getattribute__(loss_name)(**loss_params)\n","\n","\n","def get_optimizer(cfg, parameters):\n","    optimizer_name = cfg.optimizer_name\n","    optimizer_params = cfg.optimizer_params\n","\n","    return optim.__getattribute__(optimizer_name)(parameters, **optimizer_params)\n","\n","\n","def get_optimizer_params(model, encoder_lr, decoder_lr, weight_decay=0.0):\n","    # param_optimizer = list(model.named_parameters())\n","    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n","    optimizer_parameters = [\n","        {'params': [p for n, p in model.backbone.named_parameters() if not any(nd in n for nd in no_decay)],\n","            'lr': encoder_lr, 'weight_decay': weight_decay},\n","        {'params': [p for n, p in model.backbone.named_parameters() if any(nd in n for nd in no_decay)],\n","            'lr': encoder_lr, 'weight_decay': 0.0},\n","        {'params': [p for n, p in model.named_parameters() if \"backbone\" not in n],\n","            'lr': decoder_lr, 'weight_decay': 0.0}\n","    ]\n","\n","    return optimizer_parameters\n","\n","\n","def get_scheduler(cfg, optimizer, num_warmup_steps=None, num_training_steps=None):\n","    scheduler_name = cfg.scheduler_name\n","    scheduler_params = cfg.scheduler_params\n","\n","    if scheduler_name == \"cosine-warmup\":\n","        return get_cosine_schedule_with_warmup(\n","            optimizer,\n","            num_warmup_steps=num_warmup_steps,\n","            num_training_steps=num_training_steps,\n","            **scheduler_params\n","        )\n","    elif scheduler_name == \"linear-warmup\":\n","        return get_linear_schedule_with_warmup(\n","            optimizer,\n","            num_warmup_steps=num_warmup_steps,\n","            num_training_steps=num_training_steps,\n","            **scheduler_params\n","        )\n","    else:\n","        return optim.lr_scheduler.__getattribute__(scheduler_name)(optimizer, **scheduler_params)\n","\n","\n","# ====================================\n","# Train & Predict #\n","# ====================================\n","def train_fold(cfg, train_df, valid_df, tokenizer, fold, valid_labels):\n","    # Seed\n","    seed_everything(cfg.seed)\n","\n","    # Wandb\n","    wandb_logger = WandbLogger(\n","        project=cfg.competition_name,\n","        group=cfg.group,\n","        name=f\"exp{cfg.exp_id}-fold-{fold}\",\n","        job_type=\"train\",\n","        reinit=True,\n","        anonymous=\"must\",\n","    )\n","\n","    # Model Checkpoint\n","    checkpoint = ModelCheckpoint(\n","        dirpath=cfg.model_dir,\n","        # filename=f\"exp{cfg.exp_id}-fold-{fold}\" + \"-{epoch}\",\n","        filename=f\"exp{cfg.exp_id}-fold-{fold}\",\n","        **cfg.checkpoint_params,\n","    )\n","\n","    # Learning Rate\n","    lr_monitor = LearningRateMonitor(logging_interval=\"step\")\n","    callbacks = [checkpoint, lr_monitor]\n","\n","    # Early Stopping\n","    if cfg.early_stopping:\n","        early_stopping = EarlyStopping(**cfg.early_stopping_params)\n","        callbacks += [early_stopping]\n","    \n","    # DataModule\n","    lightning_datamodule = NBMEDataModule(\n","        cfg=cfg,\n","        tokenizer=tokenizer,\n","        train_df=train_df,\n","        valid_df=valid_df,\n","    )\n","\n","    # Model\n","    lightning_model = NBMELightningModule(\n","        cfg,\n","        tokenizer,\n","        valid_df,\n","        valid_labels,\n","    )\n","\n","    # Trainer\n","    trainer = Trainer(\n","        gpus=cfg.gpus,\n","        max_epochs=cfg.max_epochs,\n","        callbacks=callbacks,\n","        logger=[wandb_logger],\n","        accumulate_grad_batches=cfg.accumulate_grad_batches,\n","        precision=cfg.precision,\n","        # deterministic=True,\n","        benchmark=False,\n","    )\n","\n","    trainer.fit(lightning_model, datamodule=lightning_datamodule)\n","    wandb.finish(quiet=True)\n","\n","    del lightning_datamodule, lightning_model, trainer\n","\n","    torch.cuda.empty_cache()\n","    gc.collect()\n","\n","\n","def train_cv(cfg, input_df, tokenizer):\n","    oof_df = pd.DataFrame()\n","    for fold_id in range(cfg.num_fold):\n","        if fold_id in cfg.train_fold:\n","            filename = f\"exp{cfg.exp_id}-fold-{fold_id}\"\n","            filelist = get_filname_listdir(cfg.model_dir)\n","\n","            train_df = input_df[input_df[\"fold\"] != fold_id].reset_index(drop=True)\n","            valid_df = input_df[input_df[\"fold\"] == fold_id].reset_index(drop=True)\n","            valid_labels = create_labels_for_scoring(valid_df)\n","\n","            # training\n","            if not filename in filelist:\n","                train_fold(\n","                    cfg=cfg,\n","                    train_df=train_df,\n","                    valid_df=valid_df,\n","                    tokenizer=tokenizer,\n","                    fold=fold_id,\n","                    valid_labels=valid_labels,\n","                )\n","\n","            # oof\n","            pred_df = predict(\n","                cfg=cfg,\n","                input_df=valid_df,\n","                tokenizer=tokenizer,\n","                filename=filename,\n","                labels=valid_labels,\n","            )\n","\n","            pred_df[\"label_location\"] = valid_labels\n","            oof_score = get_score(pred_df[\"label_location\"], pred_df[\"predict_location\"])\n","            cfg.logger.info(f\"Fold: {fold_id} oof-score: {oof_score}\")\n","            oof_df = pd.concat([oof_df, pred_df], axis=0)\n","\n","    return oof_df.reset_index(drop=True)\n","\n","\n","def predict_raw_prediction(cfg, input_df, tokenizer, filename, labels=None):\n","    checkpoint_path = os.path.join(cfg.model_dir, filename + \".ckpt\")\n","\n","    lightning_model = NBMELightningModule(\n","        cfg,\n","        tokenizer,\n","        input_df,\n","        labels,\n","    )\n","\n","    lightning_model = lightning_model.load_from_checkpoint(\n","        checkpoint_path=checkpoint_path,\n","        cfg=cfg,\n","    )\n","\n","    lightning_datamodule = NBMEDataModule(\n","        cfg,\n","        tokenizer=tokenizer,\n","        test_df=input_df\n","    )\n","\n","    trainer = Trainer(\n","        gpus=cfg.gpus,\n","    )\n","\n","    preds = trainer.predict(\n","        lightning_model,\n","        datamodule=lightning_datamodule,\n","        return_predictions=True\n","    )\n","\n","    preds = torch.cat(preds).cpu().numpy() # (sample, max_seq, num_class)\n","    \n","    return preds\n","    \n","\n","def predict(cfg, input_df, tokenizer, filename, labels):\n","    file_path = os.path.join(cfg.exp_output_dir, f\"{filename}.npy\")\n","    \n","    if os.path.isfile(file_path):\n","        preds = np.load(file_path)\n","    else:\n","        preds = predict_raw_prediction(cfg, input_df, tokenizer, filename, labels)\n","        np.save(os.path.join(cfg.exp_output_dir, filename), preds)\n","\n","    char_preds = get_token_probs_to_char_probs(input_df[\"pn_history\"].to_numpy(), preds, tokenizer)\n","    results = get_results(char_preds, th=0.5)\n","    preds = get_predictions(results)\n","\n","    output_df = input_df.copy()\n","    output_df[\"predict_location\"] = preds\n","\n","    return output_df\n","\n","\n","def predict_cv(cfg, input_df, tokenizer):\n","    \"\"\"\n","    CVモデルで予測\n","    \"\"\"\n","    fold_preds = []\n","    for fold_id in range(cfg.num_fold):\n","        if fold_id in cfg.train_fold:\n","            filename = f\"exp{cfg.exp_id}-fold-{fold_id}\"\n","            preds = predict_raw_prediction(cfg, input_df, tokenizer, filename)\n","            char_preds = get_token_probs_to_char_probs(input_df[\"pn_history\"].to_numpy(), preds, tokenizer)\n","            fold_preds.append(char_preds)\n","\n","    fold_preds = np.mean(fold_preds, axis=0)\n","    results = get_results(char_preds, th=0.5)\n","\n","    output_df = input_df.copy()\n","    output_df[\"location\"] = results\n","\n","    return output_df\n","\n","\n","def get_token_probs_to_char_probs(texts, predictions, tokenizer):\n","    \"\"\"\n","    予測値をtoken-level -> char-levelに変形\n","    \"\"\"\n","    results = [np.zeros(len(t)) for t in texts]\n","    for i, (text, prediction) in enumerate(zip(texts, predictions)):\n","        encoded = tokenizer(\n","            text, \n","            add_special_tokens=True,\n","            return_offsets_mapping=True\n","        )\n","        \n","        for idx, (offset_mapping, pred) in enumerate(zip(encoded['offset_mapping'], prediction)):\n","            start = offset_mapping[0]\n","            end = offset_mapping[1]\n","\n","            # 先行するスペースがあればスパンから除く\n","            # if text[start] == \" \":\n","            #     start = start + 1\n","            \n","            results[i][start: end] = pred\n","    \n","    return results\n","\n","\n","def get_results(char_probs, th=0.5):\n","    \"\"\"\n","    \";\"区切りのスパンに変換\n","    \"\"\"\n","    results = []\n","    for char_prob in char_probs:\n","        result = np.where(char_prob >= th)[0] + 1\n","        result = [list(g) for _, g in itertools.groupby(result, key=lambda n, c=itertools.count(): n - next(c))]\n","        result = [f\"{min(r)} {max(r)}\" for r in result]\n","        result = \";\".join(result)\n","        results.append(result)\n","    \n","    return results\n","\n","\n","def get_predictions(results):\n","    \"\"\"\n","    各スパンのリストを要素とするリストに変換\n","    '3 4;7 9;12 13' -> [[3, 4], [7, 9], [12, 13]]\n","    \"\"\"\n","    predictions = []\n","    for result in results:\n","        prediction = []\n","        if result != \"\":\n","            for loc in [s.split() for s in result.split(';')]:\n","                start, end = int(loc[0]), int(loc[1])\n","                prediction.append([start, end])\n","        predictions.append(prediction)\n","    \n","    return predictions\n","\n","\n","def create_labels_for_scoring(df):\n","    # example: ['0 1', '3 4'] -> ['0 1; 3 4']\n","    df = df.copy()\n","    df['location_for_create_labels'] = [ast.literal_eval(f'[]')] * len(df)\n","    for i in range(len(df)):\n","        lst = df.loc[i, 'location']\n","        if lst:\n","            new_lst = ';'.join(lst)\n","            df.loc[i, 'location_for_create_labels'] = ast.literal_eval(f'[[\"{new_lst}\"]]')\n","    # create labels\n","    truths = []\n","    for location_list in df['location_for_create_labels'].values:\n","        truth = []\n","        if len(location_list) > 0:\n","            location = location_list[0]\n","            for loc in [s.split() for s in location.split(';')]:\n","                start, end = int(loc[0]), int(loc[1])\n","                truth.append([start, end])\n","        truths.append(truth)\n","    \n","    return truths\n","\n","\n","# ====================================\n","# Metrics #\n","# ====================================\n","def micro_f1(preds, truths):\n","    \"\"\"\n","    Micro f1 on binary arrays.\n","\n","    Args:\n","        preds (list of lists of ints): Predictions.\n","        truths (list of lists of ints): Ground truths.\n","\n","    Returns:\n","        float: f1 score.\n","    \"\"\"\n","    # Micro : aggregating over all instances\n","    preds = np.concatenate(preds)\n","    truths = np.concatenate(truths)\n","\n","    return f1_score(truths, preds)\n","\n","\n","def spans_to_binary(spans, length=None):\n","    \"\"\"\n","    Converts spans to a binary array indicating whether each character is in the span.\n","\n","    Args:\n","        spans (list of lists of two ints): Spans.\n","\n","    Returns:\n","        np array [length]: Binarized spans.\n","    \"\"\"\n","    length = np.max(spans) if length is None else length\n","    binary = np.zeros(length)\n","    for start, end in spans:\n","        binary[start:end] = 1\n","    \n","    return binary\n","\n","\n","def span_micro_f1(preds, truths):\n","    \"\"\"\n","    Micro f1 on spans.\n","\n","    Args:\n","        preds (list of lists of two ints): Prediction spans.\n","        truths (list of lists of two ints): Ground truth spans.\n","\n","    Returns:\n","        float: f1 score.\n","    \"\"\"\n","    bin_preds = []\n","    bin_truths = []\n","    for pred, truth in zip(preds, truths):\n","        if not len(pred) and not len(truth):\n","            continue\n","        length = max(np.max(pred) if len(pred) else 0, np.max(truth) if len(truth) else 0)\n","        bin_preds.append(spans_to_binary(pred, length))\n","        bin_truths.append(spans_to_binary(truth, length))\n","    \n","    return micro_f1(bin_preds, bin_truths)\n","\n","\n","def get_score(y_true, y_pred):\n","    score = span_micro_f1(y_true, y_pred)\n","\n","    return score\n","\n","\n","# ====================================\n","# Postprocess #\n","# ===================================="]},{"cell_type":"code","execution_count":5,"metadata":{"id":"rQgD_e_gTYtP","executionInfo":{"status":"ok","timestamp":1648794968365,"user_tz":-540,"elapsed":490,"user":{"displayName":"永友遥","userId":"11743586908271963047"}}},"outputs":[],"source":["def main(Config):\n","    # setup\n","    Config = setup(Config)\n","    Config.logger = Logger(Config.exp_output_dir)\n","    # load dataset\n","    train_df = get_input_data(Config, input_type=\"train\")\n","    test_df = get_input_data(Config, input_type=\"test\")\n","    submission_df = pd.read_csv(Config.sample_submission)\n","\n","    # split\n","    train_df = get_split(Config, train_df)\n","\n","    # tokenizer\n","    tokenizer = get_tokenizer(Config)\n","\n","    if not Config.inference_only:\n","        # training\n","        raw_oof_df = train_cv(\n","            cfg=Config,\n","            input_df=train_df,\n","            tokenizer=tokenizer,\n","        )\n","        # raw oof\n","        score = get_score(y_true=raw_oof_df[\"label_location\"], y_pred=raw_oof_df[\"predict_location\"])\n","        Config.logger.info(f\"score={score}\")\n","\n","    # predict\n","    raw_pred_df = predict_cv(\n","        cfg=Config,\n","        input_df=test_df,\n","        tokenizer=tokenizer,\n","    )\n","\n","    # upload output to kaggle dataset\n","    if Config.upload_from_colab:\n","        from kaggle.api.kaggle_api_extended import KaggleApi\n","\n","        def dataset_create_new(dataset_name, upload_dir):\n","            dataset_metadata = {}\n","            dataset_metadata['id'] = f'{os.environ[\"KAGGLE_USERNAME\"]}/{dataset_name}'\n","            dataset_metadata['licenses'] = [{'name': 'CC0-1.0'}]\n","            dataset_metadata['title'] = dataset_name\n","            with open(os.path.join(upload_dir, 'dataset-metadata.json'), 'w') as f:\n","                json.dump(dataset_metadata, f, indent=4)\n","            api = KaggleApi()\n","            api.authenticate()\n","            api.dataset_create_new(folder=upload_dir, convert_to_csv=False, dir_mode='tar')\n","\n","        dataset_create_new(dataset_name=f\"{Config.competition_name}-exp{Config.exp_id}\", upload_dir=Config.exp_output_dir)\n","\n","    # make submission\n","    if not Config.on_colab:\n","        raw_pred_df[[\"id\", \"location\"]].to_csv(os.path.join(Config.submission, \"submission.csv\"), index=False)\n","\n","\n","if __name__ == \"__main__\":\n","    main(Config)"]},{"cell_type":"code","source":[""],"metadata":{"id":"dS4JYVxT1wEt"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"machine_shape":"hm","name":"exp004.ipynb","provenance":[],"mount_file_id":"1jeVjeZ0KZv4zYb77mCrXzzLGPiPWVSCa","authorship_tag":"ABX9TyNuTOFgLAUj7N7NvnS9gcZN"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}